# Song Popularity Prediction

## ðŸŽµ Introduction

This project aims to predict the popularity of a song based on its audio features. The goal is to build a robust machine learning model that can accurately classify a song as popular or not, using a dataset of various audio attributes like danceability, energy, and acousticness. The evaluation metric for this challenge is the **Area Under the Curve (AUC)**.

---

## ðŸ“‚ Dataset

The dataset is provided by Kaggle and consists of two main files:
* **`train.csv`**: Contains the training data with audio features and the target variable, `song_popularity`.
* **`test.csv`**: Contains the test data with the same features as the training set, but without the target variable.

### Key Features
* **`danceability`**: How suitable a track is for dancing.
* **`energy`**: A perceptual measure of intensity and activity.
* **`loudness`**: The overall loudness of a track in decibels (dB).
* **`speechiness`**: The presence of spoken words in a track.
* **`acousticness`**: A confidence measure of whether the track is acoustic.
* **`instrumentalness`**: Predicts whether a track contains no vocals.
* **`liveness`**: Detects the presence of an audience in the recording.
* **`valence`**: The musical positiveness conveyed by a track.
* **`tempo`**: The speed of the piece in beats per minute (BPM).

---

##  Methodology

The project follows a standard machine learning pipeline:

1.  **Feature Engineering**: New features were created to capture interactions between existing ones. For instance, `danceability_X_energy` was created by multiplying `danceability` and `energy`. Statistical features based on `key`, `time_signature`, and `audio_mode` were also generated.

2.  **Data Preprocessing**:
    * **Imputation**: Missing values were handled using `IterativeImputer`, which models each feature with missing values as a function of other features.
    * **Scaling**: All features were standardized using `StandardScaler` to ensure that models are not biased by features with larger scales.

3.  **Modeling**: Three powerful gradient boosting models were trained and evaluated using 10-fold stratified cross-validation:
    * **LightGBM**
    * **XGBoost**
    * **CatBoost**

4.  **Ensembling**: The final prediction was generated by blending the predictions of the three models, averaging their outputs to create a more robust and accurate final submission.

---

## ðŸ“ˆ Results

The ensembling strategy proved effective, leveraging the strengths of each individual model. The final blended model achieved a local cross-validation AUC score that was higher than any single model, demonstrating the power of ensembling.

---

##  How to Run

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/your-username/song-popularity-prediction.git](https://github.com/your-username/song-popularity-prediction.git)
    cd song-popularity-prediction
    ```

2.  **Install the dependencies:**
    ```bash
    pip install -r scripts/requirements.txt
    ```

3.  **Run the training script:**
    ```bash
    python scripts/train.py
    ```
    This will generate the `submission_grandmaster_blend.csv` file.
